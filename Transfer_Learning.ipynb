import tensorflow as tf
import tensorflow_datasets as tfds
import matplotlib.pyplot as plt
import numpy as np

print(f"TensorFlow Version: {tf.__version__}")

# ==============================================================================
# 1. CARREGAR OS DADOS (A PARTE MAIS IMPORTANTE PARA VOCÊ)
# ==============================================================================
# O dataset 'cats_vs_dogs' original só tem um split 'train' de ~23.000 imagens.
# Vamos dividi-lo manualmente usando "slices":
#
# - Treino: Os primeiros 15% das imagens
# - Validação: Os próximos 5% (de 15% a 20%)
# - Teste: Os próximos 5% (de 20% a 25%)
#
# Isso reduzirá drasticamente o tempo de treinamento para você iterar rápido.

SPLIT_TREINO = 'train[:15%]'
SPLIT_VALID = 'train[15%:20%]'
SPLIT_TESTE = 'train[20%:25%]'

# Carrega os dados com os splits definidos
(raw_train, raw_validation, raw_test), metadata = tfds.load(
    'cats_vs_dogs',
    split=[SPLIT_TREINO, SPLIT_VALID, SPLIT_TESTE],
    with_info=True,
    as_supervised=True,  # Retorna (imagem, label) em vez de um dicionário
)

num_train = metadata.splits['train'].num_examples * 0.15
num_validation = metadata.splits['train'].num_examples * 0.05
num_test = metadata.splits['train'].num_examples * 0.05

print(f"Total de imagens de treino: ~{num_train:.0f}")
print(f"Total de imagens de validação: ~{num_validation:.0f}")
print(f"Total de imagens de teste: ~{num_test:.0f}")

# Nomes das classes (0 = Gato, 1 = Cachorro)
get_label_name = metadata.features['label'].int2str
# print(f"Classes: {get_label_name(0)}, {get_label_name(1)}")

# ==============================================================================
# 2. PRÉ-PROCESSAMENTO DOS DADOS
# ==============================================================================
# O MobileNetV2 (o modelo de transferência) espera imagens de um tamanho específico
# e com pixels normalizados de uma certa maneira.

IMG_SIZE = 160 # Tamanho de entrada esperado pelo MobileNetV2
BATCH_SIZE = 32

def format_example(image, label):
    """
    Formata a imagem:
    1. Redimensiona para IMG_SIZE x IMG_SIZE
    2. Normaliza os pixels para o intervalo [-1, 1] (esperado pelo MobileNetV2)
    """
    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))
    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
    return image, label

# Cria os pipelines de dados
train_dataset = raw_train.map(format_example).shuffle(int(num_train)).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)
validation_dataset = raw_validation.map(format_example).batch(BATCH_SIZE).prefetch(buffer_size=tf.data.AUTOTUNE)
test_dataset = raw_test.map(format_example).batch(BATCH_SIZE)

# ==============================================================================
# 3. CRIAR O MODELO DE TRANSFER LEARNING
# ==============================================================================

# Carrega o MobileNetV2 pré-treinado na ImageNet, sem a camada de classificação final
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,  # Não inclua a camada final (Dense de 1000 classes)
    weights='imagenet'
)

# Congela os pesos do modelo base
# Não queremos treiná-los, apenas usar seus "conhecimentos" (features)
base_model.trainable = False

# Vamos ver a arquitetura do modelo base
# base_model.summary()

# ==============================================================================
# 4. ADICIONAR NOSSA PRÓPRIA CAMADA DE CLASSIFICAÇÃO
# ==============================================================================
# Precisamos adicionar nossas próprias camadas no topo do modelo base para
# classificar Gatos vs. Cachorros (2 classes).

# Como é uma classificação binária (Gato ou Cachorro), usaremos 1 neurônio
# com ativação 'sigmoid'.
prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')

# Constrói o modelo final
inputs = tf.keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = base_model(inputs, training=False) # Passa a entrada pelo modelo base (congelado)
x = tf.keras.layers.GlobalAveragePooling2D()(x) # Reduz a dimensionalidade
x = tf.keras.layers.Dropout(0.2)(x) # Adiciona Dropout para regularização
outputs = prediction_layer(x) # Nossa camada de classificação final
model = tf.keras.Model(inputs, outputs)

# ==============================================================================
# 5. COMPILAR E TREINAR O MODELO
# ==============================================================================

# Compila o modelo
# Usamos 'binary_crossentropy' porque temos 2 classes e uma saída sigmoid
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

print("\nIniciando o treinamento (feature extraction)...")

EPOCHS = 10

# Treina o modelo!
# Isso será MUITO mais rápido agora com apenas 15% dos dados.
history = model.fit(
    train_dataset,
    epochs=EPOCHS,
    validation_data=validation_dataset
)

# ==============================================================================
# 6. AVALIAR OS RESULTADOS
# ==============================================================================

print("\nAvaliando o modelo no conjunto de teste...")
loss, accuracy = model.evaluate(test_dataset)
print(f"Perda no Teste: {loss:.4f}")
print(f"Acurácia no Teste: {accuracy:.4f}")

# Plotar a acurácia e a perda
acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

plt.figure(figsize=(10, 4))
plt.subplot(1, 2, 1)
plt.plot(acc, label='Acurácia de Treino')
plt.plot(val_acc, label='Acurácia de Validação')
plt.legend(loc='lower right')
plt.title('Acurácia de Treino e Validação')

plt.subplot(1, 2, 2)
plt.plot(loss, label='Perda de Treino')
plt.plot(val_loss, label='Perda de Validação')
plt.legend(loc='upper right')
plt.title('Perda de Treino e Validação')
plt.show()

# ==============================================================================
# 7. (OPCIONAL) FINE-TUNING (AJUSTE FINO)
# ==============================================================================
# Após o modelo "aquecer" (treinar as camadas de topo), podemos "descongelar"
# parte do modelo base e treiná-lo com uma taxa de aprendizado (learning rate)
# muito baixa.

print("\nIniciando o Fine-Tuning...")

# Descongela o modelo base
base_model.trainable = True

# Vamos descongelar apenas as camadas mais altas
fine_tune_at = 100 # Congela as primeiras 100 camadas, treina o resto

# Congela todas as camadas antes da camada `fine_tune_at`
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Recompila o modelo com um learning rate BEM baixo
model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.00001), # 10x menor
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

# Continua o treinamento (fine-tuning)
fine_tune_epochs = 10
total_epochs = EPOCHS + fine_tune_epochs

history_fine = model.fit(
    train_dataset,
    epochs=total_epochs,
    initial_epoch=history.epoch[-1], # Continua de onde paramos
    validation_data=validation_dataset
)

# Avaliação final
print("\nAvaliando o modelo após Fine-Tuning...")
loss, accuracy = model.evaluate(test_dataset)
print(f"Perda Final no Teste: {loss:.4f}")
print(f"Acurácia Final no Teste: {accuracy:.4f}")
